{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a3b8bcc",
   "metadata": {},
   "source": [
    "# 유사한 단어 찾기 게임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe4220a",
   "metadata": {},
   "source": [
    "1. 사전 학습된 모델 또는 적절한 데이터셋을 찾는다.\n",
    "2. 워드 임베딩 모델을 학습시킨다.\n",
    "3. 단어 유사도가 0.8 이상인 A, B를 랜덤 추출한다.\n",
    "4. A, B와 대응되는 C를 추출한다.\n",
    "5. D를 입력받는다.\n",
    "\n",
    "=>\n",
    "A:B = C:D 관계에 대응하는 D를 찾는 게임을 만든다.\n",
    "ex) A: 산, B: 바다, C: 나무, D: 물"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9a9c11",
   "metadata": {},
   "source": [
    "**<출력 예시>**\n",
    "\n",
    "관계 [수긍: 추락 = 대사관 : ?]\n",
    "\n",
    "모델이 예측한 가장 적합한 단어: 잠입\n",
    "\n",
    "당신의 답변과 모델 예측의 유사도: 0.34\n",
    "\n",
    "아쉽네요. 더 생각해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0bec48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# ▶ 파일 경로 (필요시 수정)\n",
    "STOPWORDS_PATH = Path(\"ko_stopwords.txt\")\n",
    "TRANSCRIPT_PATH = Path(\"transcript.v.1.4.txt\")\n",
    "EMB_PATH = Path(\"cc.ko.300.vec\")\n",
    "\n",
    "# ▶ 옵션\n",
    "USE_EMBEDDING_FILTER = True   # 모델 사전에 있는 단어만 남기려면 True\n",
    "SHOW_ROWS = 5                  # 미리보기 개수\n",
    "MIN_TOKEN_LEN = 2              # 1글자 토큰 제거\n",
    "ALLOW_POS = (\"N\",)  # 명사/동사/형용사만\n",
    "MIN_FREQ = 1                   # 후보 단어 풀 최소 빈도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90563a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, unicodedata\n",
    "from collections import Counter\n",
    "from typing import List, Set, Iterable, Optional, Tuple, Dict\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "RE_URL     = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "RE_EMAIL   = re.compile(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\")\n",
    "RE_HTML    = re.compile(r\"<[^>]+>\")\n",
    "RE_EMOJI   = re.compile(\"[\\U00010000-\\U0010FFFF]\", flags=re.UNICODE)\n",
    "RE_HANJA = re.compile(r\"[\\u3400-\\u4DBF\\u4E00-\\u9FFF\\uF900-\\uFAFF]\")\n",
    "RE_ETC     = re.compile(r\"[^가-힣A-Za-z0-9\\s]\")   # 한글/영문/숫자/공백만 유지\n",
    "RE_NUMONLY = re.compile(r\"^\\d+$\")\n",
    "RE_MULTIWS = re.compile(r\"\\s+\")\n",
    "BAD_TOKENS = {\"</s>\", \"_\", \"%\", \"…\", \"·\", \"”\", \"“\", \"’\", \"‘\", \"—\", \"-\", \"–\"}\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    s = unicodedata.normalize(\"NFKC\", s)\n",
    "    s = RE_HTML.sub(\" \", s)\n",
    "    s = RE_URL.sub(\" \", s)\n",
    "    s = RE_EMAIL.sub(\" \", s)\n",
    "    s = RE_EMOJI.sub(\" \", s)\n",
    "    s = RE_HANJA.sub(\" \", s)\n",
    "    s = RE_ETC.sub(\" \", s)\n",
    "    s = RE_MULTIWS.sub(\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def is_korean_word(w: str) -> bool:\n",
    "    return bool(re.match(r\"^[가-힣]{2,}$\", w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b3eb123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 수: 498\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\t', '가', '가까스로', '가다', '가령', '각', '각각', '각자', '각종', '각하']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_stopwords(path: Path) -> Set[str]:\n",
    "    if not path.exists():\n",
    "        print(f\"[경고] 불용어 파일이 없습니다: {path}\")\n",
    "        return set()\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        raw = [line.strip() for line in f if line.strip() and not line.startswith(\"#\")]\n",
    "    # 기본형으로 정규화\n",
    "    normed = set()\n",
    "    for w in raw:\n",
    "        # 단어 단독 토큰화 → 기본형 추출\n",
    "        morphs = okt.pos(w, norm=True, stem=True)\n",
    "        for token, pos in morphs:\n",
    "            if len(token) >= 1:\n",
    "                normed.add(token)\n",
    "    return normed\n",
    "\n",
    "custom_stopwords: Set[str] = load_stopwords(STOPWORDS_PATH)\n",
    "print(\"불용어 수:\", len(custom_stopwords))\n",
    "list(sorted(list(custom_stopwords)))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c9e4e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 수: 12854\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['그는 괜찮은 척하려고 애쓰는 것 같았다.',\n",
       " '그녀의 사랑을 얻기 위해 애썼지만 헛수고였다.',\n",
       " '용돈을 아껴 써라.',\n",
       " '그는 아내를 많이 아낀다.',\n",
       " '그 애 전화번호 알아?']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_korean_sentences_from_transcript(path: Path) -> List[str]:\n",
    "    sentences: List[str] = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.rstrip(\"\\n\").split(\"|\")\n",
    "            if len(parts) > 1:\n",
    "                ko = parts[1].strip()\n",
    "                if ko:\n",
    "                    sentences.append(ko)\n",
    "    return sentences\n",
    "\n",
    "raw_sentences: List[str] = load_korean_sentences_from_transcript(TRANSCRIPT_PATH)\n",
    "print(\"문장 수:\", len(raw_sentences))\n",
    "raw_sentences[:SHOW_ROWS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6d200f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임베딩 로드 중… cc.ko.300.vec\n",
      "임베딩 단어 수: 1999989\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "kv = None\n",
    "if USE_EMBEDDING_FILTER:\n",
    "    print(\"임베딩 로드 중…\", EMB_PATH)\n",
    "    kv = KeyedVectors.load_word2vec_format(\n",
    "        str(EMB_PATH), binary=False, encoding=\"utf-8\", unicode_errors=\"ignore\"\n",
    "    )\n",
    "    print(\"임베딩 단어 수:\", len(kv.key_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a50a82e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_ko(\n",
    "    s: str,\n",
    "    stopwords: Set[str],\n",
    "    model: Optional[KeyedVectors] = None,\n",
    "    min_len: int = MIN_TOKEN_LEN,\n",
    "    allow_pos: Tuple[str, ...] = ALLOW_POS\n",
    ") -> List[str]:\n",
    "    s = normalize_text(s)\n",
    "    morphs = okt.pos(s, norm=True, stem=True)\n",
    "    keep: List[str] = []\n",
    "    for w, p in morphs:\n",
    "        if w in BAD_TOKENS: continue\n",
    "        if len(w) < min_len: continue\n",
    "        if RE_NUMONLY.match(w): continue\n",
    "        if w in stopwords: continue\n",
    "        if not p.startswith(allow_pos): continue\n",
    "        if model is not None and w not in model.key_to_index: continue\n",
    "        keep.append(w)\n",
    "    keep = [w for w in keep if is_korean_word(w)]\n",
    "    return keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2db6c6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰화된 문장 수: 11377\n",
      "고유 토큰 수: 5004\n",
      "후보 단어 풀 크기: 5004\n",
      "\n",
      "[샘플 토큰화 결과]\n",
      "1. ['그녀', '사랑', '수고']\n",
      "2. ['용돈']\n",
      "3. ['아내']\n",
      "4. ['전화번호']\n",
      "5. ['거기']\n",
      "\n",
      "[빈도 상위 20]\n",
      "그녀 238\n",
      "오늘 216\n",
      "한국 164\n",
      "지금 140\n",
      "정말 131\n",
      "문제 128\n",
      "여자 123\n",
      "요즘 113\n",
      "어제 112\n",
      "회사 110\n",
      "엄마 103\n",
      "친구 101\n",
      "아내 100\n",
      "아주 100\n",
      "모든 86\n",
      "아버지 84\n",
      "남자 78\n",
      "수가 78\n",
      "영화 76\n",
      "마음 76\n",
      "\n",
      "[후보 단어 풀 상위 30]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['그녀',\n",
       " '사랑',\n",
       " '수고',\n",
       " '용돈',\n",
       " '아내',\n",
       " '전화번호',\n",
       " '거기',\n",
       " '시험',\n",
       " '감기',\n",
       " '사흘',\n",
       " '몸살',\n",
       " '요즘',\n",
       " '공부',\n",
       " '장사',\n",
       " '아기',\n",
       " '안고',\n",
       " '엄마',\n",
       " '자리',\n",
       " '여자',\n",
       " '전화',\n",
       " '번호',\n",
       " '절대',\n",
       " '의견',\n",
       " '영어',\n",
       " '동료',\n",
       " '수화',\n",
       " '색깔',\n",
       " '소영이',\n",
       " '얼음',\n",
       " '핸드폰']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰화\n",
    "tokenized_corpus: List[List[str]] = [\n",
    "    tokenize_ko(s, stopwords=custom_stopwords, model=kv) for s in raw_sentences\n",
    "]\n",
    "# 빈/짧은 문장 제거\n",
    "tokenized_corpus = [t for t in tokenized_corpus if len(t) >= 1]\n",
    "\n",
    "# 빈도 & 후보 단어 풀\n",
    "freq = Counter([w for sent in tokenized_corpus for w in sent])\n",
    "candidate_vocab = [w for w, c in freq.items() if c >= MIN_FREQ and is_korean_word(w)]\n",
    "\n",
    "print(\"토큰화된 문장 수:\", len(tokenized_corpus))\n",
    "print(\"고유 토큰 수:\", len(freq))\n",
    "print(\"후보 단어 풀 크기:\", len(candidate_vocab))\n",
    "\n",
    "# 미리보기\n",
    "print(\"\\n[샘플 토큰화 결과]\")\n",
    "for i in range(min(SHOW_ROWS, len(tokenized_corpus))):\n",
    "    print(f\"{i+1}.\", tokenized_corpus[i])\n",
    "\n",
    "print(\"\\n[빈도 상위 20]\")\n",
    "for w, c in freq.most_common(20):\n",
    "    print(w, c)\n",
    "\n",
    "print(\"\\n[후보 단어 풀 상위 30]\")\n",
    "candidate_vocab[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdbca634",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nlp_env/lib/python3.12/site-packages/gensim/models/keyedvectors.py:849: RuntimeWarning: invalid value encountered in divide\n",
      "  dists = dot(self.vectors[clip_start:clip_end], mean) / self.norms[clip_start:clip_end]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유사도 ≥ 0.8 쌍 개수: 26\n",
      "[('관해', '대해', 0.8129411935806274), ('점점', '점차', 0.8261826038360596), ('적도', '적이', 0.8028584122657776), ('오전', '오후', 0.9102688431739807), ('둘째', '첫째', 0.8528753519058228), ('일요일', '토요일', 0.8150652647018433), ('눈코', '뜰새', 0.8282690048217773), ('감소', '증가', 0.8052759170532227), ('수가', '수는', 0.8367935419082642), ('남자', '여자', 0.8801066279411316)]\n",
      "pairs 내 비한글 쌍 수: 0\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from gensim.matutils import unitvec\n",
    "\n",
    "def cosine(u, v) -> float:\n",
    "    return float(np.dot(unitvec(u), unitvec(v)))\n",
    "\n",
    "# 1) similar_pairs_over 보강: a, b 모두 한글만\n",
    "def similar_pairs_over(vec: KeyedVectors, words, thr: float = 0.8, per_word_topn: int = 50, max_pairs: int = 500):\n",
    "    # a 후보: 임베딩에 있고 한글만\n",
    "    words = [w for w in words if (w in vec) and is_korean_word(w)]\n",
    "    pairs = set()\n",
    "    random.shuffle(words)\n",
    "\n",
    "    for a in words:\n",
    "        try:\n",
    "            nbrs = vec.most_similar(a, topn=per_word_topn)\n",
    "        except KeyError:\n",
    "            continue\n",
    "        for b, s in nbrs:\n",
    "            if s < thr or b == a:\n",
    "                continue\n",
    "            if (b not in vec) or (not is_korean_word(b)):   # ← b도 한글만\n",
    "                continue\n",
    "            u = tuple(sorted((a, b)))\n",
    "            if u not in pairs:\n",
    "                pairs.add(u)\n",
    "                if len(pairs) >= max_pairs:\n",
    "                    return [(x[0], x[1], cosine(vec[x[0]], vec[x[1]])) for x in pairs]\n",
    "    return [(x[0], x[1], cosine(vec[x[0]], vec[x[1]])) for x in pairs]\n",
    "\n",
    "# 2) 기존 pairs가 있으면 한 번 더 걸러주기\n",
    "try:\n",
    "    pairs\n",
    "    pairs = [(a, b, c) for (a, b, c) in pairs if is_korean_word(a) and is_korean_word(b)]\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "# 3) 새로 유사쌍 생성\n",
    "pairs = similar_pairs_over(kv, candidate_vocab, thr=0.8, per_word_topn=50, max_pairs=500)\n",
    "print(\"유사도 ≥ 0.8 쌍 개수:\", len(pairs))\n",
    "print(pairs[:10])\n",
    "\n",
    "# 4) 검증: pairs에 비한글이 남아있는지 확인\n",
    "nonko_in_pairs = [(a,b) for a,b,_ in pairs if not (is_korean_word(a) and is_korean_word(b))]\n",
    "print(\"pairs 내 비한글 쌍 수:\", len(nonko_in_pairs))\n",
    "if nonko_in_pairs:\n",
    "    print(nonko_in_pairs[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3257751c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_analogy(vec: KeyedVectors, a: str, b: str, c: str, vocab):\n",
    "    # gensim 방식: positive=[b, c], negative=[a]\n",
    "    cand = vec.most_similar(positive=[b, c], negative=[a], topn=100)\n",
    "    for w, s in cand:\n",
    "        if w not in {a, b, c} and w in vocab and is_korean_word(w):\n",
    "            return w, float(s)\n",
    "    # 백업: 직접 계산\n",
    "    dvec = vec[b] - vec[a] + vec[c]\n",
    "    sims = []\n",
    "    for w in vocab:\n",
    "        if w in {a, b, c}: \n",
    "            continue\n",
    "        sims.append((w, cosine(vec[w], dvec)))\n",
    "    sims.sort(key=lambda x: x[1], reverse=True)\n",
    "    return sims[0] if sims else (None, 0.0)\n",
    "\n",
    "def pick_C(vec: KeyedVectors, a: str, b: str, vocab):\n",
    "    # A 근방에서 B와 너무 가깝지 않은 후보를 선호\n",
    "    try:\n",
    "        nearA = [w for w, s in vec.most_similar(a, topn=100) if w in vocab and w not in {a, b}]\n",
    "    except KeyError:\n",
    "        nearA = []\n",
    "    random.shuffle(nearA)\n",
    "    for w in nearA:\n",
    "        try:\n",
    "            if vec.similarity(w, b) < 0.6:\n",
    "                return w\n",
    "        except KeyError:\n",
    "            continue\n",
    "    # fallback\n",
    "    rest = [w for w in vocab if w not in {a, b}]\n",
    "    return random.choice(rest) if rest else None\n",
    "\n",
    "def make_round(vec: KeyedVectors, pairs_pool, vocab):\n",
    "    a, b, _ = random.choice(pairs_pool)\n",
    "    c = pick_C(vec, a, b, vocab)\n",
    "    d_pred, conf = best_analogy(vec, a, b, c, vocab)\n",
    "    return {\"A\": a, \"B\": b, \"C\": c, \"D_pred\": d_pred, \"conf\": round(conf, 3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c417cff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_round(rnd: dict) -> str:\n",
    "    return f\"관계 [{rnd['A']}: {rnd['B']} = {rnd['C']} : ?]\"\n",
    "\n",
    "def grade_answer(vec: KeyedVectors, rnd: dict, user_answer: str) -> str:\n",
    "    if rnd[\"D_pred\"] is None or user_answer not in vec:\n",
    "        sim = 0.0\n",
    "    else:\n",
    "        sim = cosine(vec[rnd[\"D_pred\"]], vec[user_answer])\n",
    "    msg = [\n",
    "        render_round(rnd),\n",
    "        f\"모델이 예측한 가장 적합한 단어: {rnd['D_pred']}\",\n",
    "        f\"당신의 답변과 모델 예측의 유사도: {sim:.2f}\",\n",
    "    ]\n",
    "    if sim >= 0.7:\n",
    "        msg.append(\"정답에 매우 가깝습니다! 🎯\")\n",
    "    elif sim >= 0.4:\n",
    "        msg.append(\"오, 거의 근접했어요. 한 번만 더!\")\n",
    "    else:\n",
    "        msg.append(\"아쉽네요. 더 생각해보세요.\")\n",
    "    return \"\\n\\n\".join(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e5cf4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "관계 [남자: 여자 = 남성 : ?]\n",
      "\n",
      "관계 [남자: 여자 = 남성 : ?]\n",
      "\n",
      "모델이 예측한 가장 적합한 단어: 여성\n",
      "\n",
      "당신의 답변과 모델 예측의 유사도: 1.00\n",
      "\n",
      "정답에 매우 가깝습니다! 🎯\n"
     ]
    }
   ],
   "source": [
    "rnd = make_round(kv, pairs, candidate_vocab)\n",
    "print(render_round(rnd))\n",
    "\n",
    "user_answer = input(\"당신의 답은? \").strip()\n",
    "print(\"\\n\" + grade_answer(kv, rnd, user_answer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
