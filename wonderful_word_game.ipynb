{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a3b8bcc",
   "metadata": {},
   "source": [
    "# ìœ ì‚¬í•œ ë‹¨ì–´ ì°¾ê¸° ê²Œì„"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe4220a",
   "metadata": {},
   "source": [
    "1. ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ë˜ëŠ” ì ì ˆí•œ ë°ì´í„°ì…‹ì„ ì°¾ëŠ”ë‹¤.\n",
    "2. ì›Œë“œ ì„ë² ë”© ëª¨ë¸ì„ í•™ìŠµì‹œí‚¨ë‹¤.\n",
    "3. ë‹¨ì–´ ìœ ì‚¬ë„ê°€ 0.8 ì´ìƒì¸ A, Bë¥¼ ëœë¤ ì¶”ì¶œí•œë‹¤.\n",
    "4. A, Bì™€ ëŒ€ì‘ë˜ëŠ” Cë¥¼ ì¶”ì¶œí•œë‹¤.\n",
    "5. Dë¥¼ ì…ë ¥ë°›ëŠ”ë‹¤.\n",
    "\n",
    "=>\n",
    "A:B = C:D ê´€ê³„ì— ëŒ€ì‘í•˜ëŠ” Dë¥¼ ì°¾ëŠ” ê²Œì„ì„ ë§Œë“ ë‹¤.\n",
    "ex) A: ì‚°, B: ë°”ë‹¤, C: ë‚˜ë¬´, D: ë¬¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9a9c11",
   "metadata": {},
   "source": [
    "**<ì¶œë ¥ ì˜ˆì‹œ>**\n",
    "\n",
    "ê´€ê³„ [ìˆ˜ê¸: ì¶”ë½ = ëŒ€ì‚¬ê´€ : ?]\n",
    "\n",
    "ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ê°€ì¥ ì í•©í•œ ë‹¨ì–´: ì ì…\n",
    "\n",
    "ë‹¹ì‹ ì˜ ë‹µë³€ê³¼ ëª¨ë¸ ì˜ˆì¸¡ì˜ ìœ ì‚¬ë„: 0.34\n",
    "\n",
    "ì•„ì‰½ë„¤ìš”. ë” ìƒê°í•´ë³´ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0bec48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# â–¶ íŒŒì¼ ê²½ë¡œ (í•„ìš”ì‹œ ìˆ˜ì •)\n",
    "STOPWORDS_PATH = Path(\"ko_stopwords.txt\")\n",
    "TRANSCRIPT_PATH = Path(\"transcript.v.1.4.txt\")\n",
    "EMB_PATH = Path(\"cc.ko.300.vec\")\n",
    "\n",
    "# â–¶ ì˜µì…˜\n",
    "USE_EMBEDDING_FILTER = True   # ëª¨ë¸ ì‚¬ì „ì— ìˆëŠ” ë‹¨ì–´ë§Œ ë‚¨ê¸°ë ¤ë©´ True\n",
    "SHOW_ROWS = 5                  # ë¯¸ë¦¬ë³´ê¸° ê°œìˆ˜\n",
    "MIN_TOKEN_LEN = 2              # 1ê¸€ì í† í° ì œê±°\n",
    "ALLOW_POS = (\"N\",)  # ëª…ì‚¬/ë™ì‚¬/í˜•ìš©ì‚¬ë§Œ\n",
    "MIN_FREQ = 1                   # í›„ë³´ ë‹¨ì–´ í’€ ìµœì†Œ ë¹ˆë„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90563a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, unicodedata\n",
    "from collections import Counter\n",
    "from typing import List, Set, Iterable, Optional, Tuple, Dict\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "RE_URL     = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "RE_EMAIL   = re.compile(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\")\n",
    "RE_HTML    = re.compile(r\"<[^>]+>\")\n",
    "RE_EMOJI   = re.compile(\"[\\U00010000-\\U0010FFFF]\", flags=re.UNICODE)\n",
    "RE_HANJA = re.compile(r\"[\\u3400-\\u4DBF\\u4E00-\\u9FFF\\uF900-\\uFAFF]\")\n",
    "RE_ETC     = re.compile(r\"[^ê°€-í£A-Za-z0-9\\s]\")   # í•œê¸€/ì˜ë¬¸/ìˆ«ì/ê³µë°±ë§Œ ìœ ì§€\n",
    "RE_NUMONLY = re.compile(r\"^\\d+$\")\n",
    "RE_MULTIWS = re.compile(r\"\\s+\")\n",
    "BAD_TOKENS = {\"</s>\", \"_\", \"%\", \"â€¦\", \"Â·\", \"â€\", \"â€œ\", \"â€™\", \"â€˜\", \"â€”\", \"-\", \"â€“\"}\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    s = unicodedata.normalize(\"NFKC\", s)\n",
    "    s = RE_HTML.sub(\" \", s)\n",
    "    s = RE_URL.sub(\" \", s)\n",
    "    s = RE_EMAIL.sub(\" \", s)\n",
    "    s = RE_EMOJI.sub(\" \", s)\n",
    "    s = RE_HANJA.sub(\" \", s)\n",
    "    s = RE_ETC.sub(\" \", s)\n",
    "    s = RE_MULTIWS.sub(\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def is_korean_word(w: str) -> bool:\n",
    "    return bool(re.match(r\"^[ê°€-í£]{2,}$\", w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b3eb123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¶ˆìš©ì–´ ìˆ˜: 498\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\t', 'ê°€', 'ê°€ê¹ŒìŠ¤ë¡œ', 'ê°€ë‹¤', 'ê°€ë ¹', 'ê°', 'ê°ê°', 'ê°ì', 'ê°ì¢…', 'ê°í•˜']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_stopwords(path: Path) -> Set[str]:\n",
    "    if not path.exists():\n",
    "        print(f\"[ê²½ê³ ] ë¶ˆìš©ì–´ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {path}\")\n",
    "        return set()\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        raw = [line.strip() for line in f if line.strip() and not line.startswith(\"#\")]\n",
    "    # ê¸°ë³¸í˜•ìœ¼ë¡œ ì •ê·œí™”\n",
    "    normed = set()\n",
    "    for w in raw:\n",
    "        # ë‹¨ì–´ ë‹¨ë… í† í°í™” â†’ ê¸°ë³¸í˜• ì¶”ì¶œ\n",
    "        morphs = okt.pos(w, norm=True, stem=True)\n",
    "        for token, pos in morphs:\n",
    "            if len(token) >= 1:\n",
    "                normed.add(token)\n",
    "    return normed\n",
    "\n",
    "custom_stopwords: Set[str] = load_stopwords(STOPWORDS_PATH)\n",
    "print(\"ë¶ˆìš©ì–´ ìˆ˜:\", len(custom_stopwords))\n",
    "list(sorted(list(custom_stopwords)))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c9e4e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¬¸ì¥ ìˆ˜: 12854\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ê·¸ëŠ” ê´œì°®ì€ ì²™í•˜ë ¤ê³  ì• ì“°ëŠ” ê²ƒ ê°™ì•˜ë‹¤.',\n",
       " 'ê·¸ë…€ì˜ ì‚¬ë‘ì„ ì–»ê¸° ìœ„í•´ ì• ì¼ì§€ë§Œ í—›ìˆ˜ê³ ì˜€ë‹¤.',\n",
       " 'ìš©ëˆì„ ì•„ê»´ ì¨ë¼.',\n",
       " 'ê·¸ëŠ” ì•„ë‚´ë¥¼ ë§ì´ ì•„ë‚€ë‹¤.',\n",
       " 'ê·¸ ì•  ì „í™”ë²ˆí˜¸ ì•Œì•„?']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_korean_sentences_from_transcript(path: Path) -> List[str]:\n",
    "    sentences: List[str] = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.rstrip(\"\\n\").split(\"|\")\n",
    "            if len(parts) > 1:\n",
    "                ko = parts[1].strip()\n",
    "                if ko:\n",
    "                    sentences.append(ko)\n",
    "    return sentences\n",
    "\n",
    "raw_sentences: List[str] = load_korean_sentences_from_transcript(TRANSCRIPT_PATH)\n",
    "print(\"ë¬¸ì¥ ìˆ˜:\", len(raw_sentences))\n",
    "raw_sentences[:SHOW_ROWS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6d200f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì„ë² ë”© ë¡œë“œ ì¤‘â€¦ cc.ko.300.vec\n",
      "ì„ë² ë”© ë‹¨ì–´ ìˆ˜: 1999989\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "kv = None\n",
    "if USE_EMBEDDING_FILTER:\n",
    "    print(\"ì„ë² ë”© ë¡œë“œ ì¤‘â€¦\", EMB_PATH)\n",
    "    kv = KeyedVectors.load_word2vec_format(\n",
    "        str(EMB_PATH), binary=False, encoding=\"utf-8\", unicode_errors=\"ignore\"\n",
    "    )\n",
    "    print(\"ì„ë² ë”© ë‹¨ì–´ ìˆ˜:\", len(kv.key_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a50a82e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_ko(\n",
    "    s: str,\n",
    "    stopwords: Set[str],\n",
    "    model: Optional[KeyedVectors] = None,\n",
    "    min_len: int = MIN_TOKEN_LEN,\n",
    "    allow_pos: Tuple[str, ...] = ALLOW_POS\n",
    ") -> List[str]:\n",
    "    s = normalize_text(s)\n",
    "    morphs = okt.pos(s, norm=True, stem=True)\n",
    "    keep: List[str] = []\n",
    "    for w, p in morphs:\n",
    "        if w in BAD_TOKENS: continue\n",
    "        if len(w) < min_len: continue\n",
    "        if RE_NUMONLY.match(w): continue\n",
    "        if w in stopwords: continue\n",
    "        if not p.startswith(allow_pos): continue\n",
    "        if model is not None and w not in model.key_to_index: continue\n",
    "        keep.append(w)\n",
    "    keep = [w for w in keep if is_korean_word(w)]\n",
    "    return keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2db6c6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í† í°í™”ëœ ë¬¸ì¥ ìˆ˜: 11377\n",
      "ê³ ìœ  í† í° ìˆ˜: 5004\n",
      "í›„ë³´ ë‹¨ì–´ í’€ í¬ê¸°: 5004\n",
      "\n",
      "[ìƒ˜í”Œ í† í°í™” ê²°ê³¼]\n",
      "1. ['ê·¸ë…€', 'ì‚¬ë‘', 'ìˆ˜ê³ ']\n",
      "2. ['ìš©ëˆ']\n",
      "3. ['ì•„ë‚´']\n",
      "4. ['ì „í™”ë²ˆí˜¸']\n",
      "5. ['ê±°ê¸°']\n",
      "\n",
      "[ë¹ˆë„ ìƒìœ„ 20]\n",
      "ê·¸ë…€ 238\n",
      "ì˜¤ëŠ˜ 216\n",
      "í•œêµ­ 164\n",
      "ì§€ê¸ˆ 140\n",
      "ì •ë§ 131\n",
      "ë¬¸ì œ 128\n",
      "ì—¬ì 123\n",
      "ìš”ì¦˜ 113\n",
      "ì–´ì œ 112\n",
      "íšŒì‚¬ 110\n",
      "ì—„ë§ˆ 103\n",
      "ì¹œêµ¬ 101\n",
      "ì•„ë‚´ 100\n",
      "ì•„ì£¼ 100\n",
      "ëª¨ë“  86\n",
      "ì•„ë²„ì§€ 84\n",
      "ë‚¨ì 78\n",
      "ìˆ˜ê°€ 78\n",
      "ì˜í™” 76\n",
      "ë§ˆìŒ 76\n",
      "\n",
      "[í›„ë³´ ë‹¨ì–´ í’€ ìƒìœ„ 30]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ê·¸ë…€',\n",
       " 'ì‚¬ë‘',\n",
       " 'ìˆ˜ê³ ',\n",
       " 'ìš©ëˆ',\n",
       " 'ì•„ë‚´',\n",
       " 'ì „í™”ë²ˆí˜¸',\n",
       " 'ê±°ê¸°',\n",
       " 'ì‹œí—˜',\n",
       " 'ê°ê¸°',\n",
       " 'ì‚¬í˜',\n",
       " 'ëª¸ì‚´',\n",
       " 'ìš”ì¦˜',\n",
       " 'ê³µë¶€',\n",
       " 'ì¥ì‚¬',\n",
       " 'ì•„ê¸°',\n",
       " 'ì•ˆê³ ',\n",
       " 'ì—„ë§ˆ',\n",
       " 'ìë¦¬',\n",
       " 'ì—¬ì',\n",
       " 'ì „í™”',\n",
       " 'ë²ˆí˜¸',\n",
       " 'ì ˆëŒ€',\n",
       " 'ì˜ê²¬',\n",
       " 'ì˜ì–´',\n",
       " 'ë™ë£Œ',\n",
       " 'ìˆ˜í™”',\n",
       " 'ìƒ‰ê¹”',\n",
       " 'ì†Œì˜ì´',\n",
       " 'ì–¼ìŒ',\n",
       " 'í•¸ë“œí°']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# í† í°í™”\n",
    "tokenized_corpus: List[List[str]] = [\n",
    "    tokenize_ko(s, stopwords=custom_stopwords, model=kv) for s in raw_sentences\n",
    "]\n",
    "# ë¹ˆ/ì§§ì€ ë¬¸ì¥ ì œê±°\n",
    "tokenized_corpus = [t for t in tokenized_corpus if len(t) >= 1]\n",
    "\n",
    "# ë¹ˆë„ & í›„ë³´ ë‹¨ì–´ í’€\n",
    "freq = Counter([w for sent in tokenized_corpus for w in sent])\n",
    "candidate_vocab = [w for w, c in freq.items() if c >= MIN_FREQ and is_korean_word(w)]\n",
    "\n",
    "print(\"í† í°í™”ëœ ë¬¸ì¥ ìˆ˜:\", len(tokenized_corpus))\n",
    "print(\"ê³ ìœ  í† í° ìˆ˜:\", len(freq))\n",
    "print(\"í›„ë³´ ë‹¨ì–´ í’€ í¬ê¸°:\", len(candidate_vocab))\n",
    "\n",
    "# ë¯¸ë¦¬ë³´ê¸°\n",
    "print(\"\\n[ìƒ˜í”Œ í† í°í™” ê²°ê³¼]\")\n",
    "for i in range(min(SHOW_ROWS, len(tokenized_corpus))):\n",
    "    print(f\"{i+1}.\", tokenized_corpus[i])\n",
    "\n",
    "print(\"\\n[ë¹ˆë„ ìƒìœ„ 20]\")\n",
    "for w, c in freq.most_common(20):\n",
    "    print(w, c)\n",
    "\n",
    "print(\"\\n[í›„ë³´ ë‹¨ì–´ í’€ ìƒìœ„ 30]\")\n",
    "candidate_vocab[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdbca634",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nlp_env/lib/python3.12/site-packages/gensim/models/keyedvectors.py:849: RuntimeWarning: invalid value encountered in divide\n",
      "  dists = dot(self.vectors[clip_start:clip_end], mean) / self.norms[clip_start:clip_end]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìœ ì‚¬ë„ â‰¥ 0.8 ìŒ ê°œìˆ˜: 26\n",
      "[('ê´€í•´', 'ëŒ€í•´', 0.8129411935806274), ('ì ì ', 'ì ì°¨', 0.8261826038360596), ('ì ë„', 'ì ì´', 0.8028584122657776), ('ì˜¤ì „', 'ì˜¤í›„', 0.9102688431739807), ('ë‘˜ì§¸', 'ì²«ì§¸', 0.8528753519058228), ('ì¼ìš”ì¼', 'í† ìš”ì¼', 0.8150652647018433), ('ëˆˆì½”', 'ëœ°ìƒˆ', 0.8282690048217773), ('ê°ì†Œ', 'ì¦ê°€', 0.8052759170532227), ('ìˆ˜ê°€', 'ìˆ˜ëŠ”', 0.8367935419082642), ('ë‚¨ì', 'ì—¬ì', 0.8801066279411316)]\n",
      "pairs ë‚´ ë¹„í•œê¸€ ìŒ ìˆ˜: 0\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from gensim.matutils import unitvec\n",
    "\n",
    "def cosine(u, v) -> float:\n",
    "    return float(np.dot(unitvec(u), unitvec(v)))\n",
    "\n",
    "# 1) similar_pairs_over ë³´ê°•: a, b ëª¨ë‘ í•œê¸€ë§Œ\n",
    "def similar_pairs_over(vec: KeyedVectors, words, thr: float = 0.8, per_word_topn: int = 50, max_pairs: int = 500):\n",
    "    # a í›„ë³´: ì„ë² ë”©ì— ìˆê³  í•œê¸€ë§Œ\n",
    "    words = [w for w in words if (w in vec) and is_korean_word(w)]\n",
    "    pairs = set()\n",
    "    random.shuffle(words)\n",
    "\n",
    "    for a in words:\n",
    "        try:\n",
    "            nbrs = vec.most_similar(a, topn=per_word_topn)\n",
    "        except KeyError:\n",
    "            continue\n",
    "        for b, s in nbrs:\n",
    "            if s < thr or b == a:\n",
    "                continue\n",
    "            if (b not in vec) or (not is_korean_word(b)):   # â† bë„ í•œê¸€ë§Œ\n",
    "                continue\n",
    "            u = tuple(sorted((a, b)))\n",
    "            if u not in pairs:\n",
    "                pairs.add(u)\n",
    "                if len(pairs) >= max_pairs:\n",
    "                    return [(x[0], x[1], cosine(vec[x[0]], vec[x[1]])) for x in pairs]\n",
    "    return [(x[0], x[1], cosine(vec[x[0]], vec[x[1]])) for x in pairs]\n",
    "\n",
    "# 2) ê¸°ì¡´ pairsê°€ ìˆìœ¼ë©´ í•œ ë²ˆ ë” ê±¸ëŸ¬ì£¼ê¸°\n",
    "try:\n",
    "    pairs\n",
    "    pairs = [(a, b, c) for (a, b, c) in pairs if is_korean_word(a) and is_korean_word(b)]\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "# 3) ìƒˆë¡œ ìœ ì‚¬ìŒ ìƒì„±\n",
    "pairs = similar_pairs_over(kv, candidate_vocab, thr=0.8, per_word_topn=50, max_pairs=500)\n",
    "print(\"ìœ ì‚¬ë„ â‰¥ 0.8 ìŒ ê°œìˆ˜:\", len(pairs))\n",
    "print(pairs[:10])\n",
    "\n",
    "# 4) ê²€ì¦: pairsì— ë¹„í•œê¸€ì´ ë‚¨ì•„ìˆëŠ”ì§€ í™•ì¸\n",
    "nonko_in_pairs = [(a,b) for a,b,_ in pairs if not (is_korean_word(a) and is_korean_word(b))]\n",
    "print(\"pairs ë‚´ ë¹„í•œê¸€ ìŒ ìˆ˜:\", len(nonko_in_pairs))\n",
    "if nonko_in_pairs:\n",
    "    print(nonko_in_pairs[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3257751c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_analogy(vec: KeyedVectors, a: str, b: str, c: str, vocab):\n",
    "    # gensim ë°©ì‹: positive=[b, c], negative=[a]\n",
    "    cand = vec.most_similar(positive=[b, c], negative=[a], topn=100)\n",
    "    for w, s in cand:\n",
    "        if w not in {a, b, c} and w in vocab and is_korean_word(w):\n",
    "            return w, float(s)\n",
    "    # ë°±ì—…: ì§ì ‘ ê³„ì‚°\n",
    "    dvec = vec[b] - vec[a] + vec[c]\n",
    "    sims = []\n",
    "    for w in vocab:\n",
    "        if w in {a, b, c}: \n",
    "            continue\n",
    "        sims.append((w, cosine(vec[w], dvec)))\n",
    "    sims.sort(key=lambda x: x[1], reverse=True)\n",
    "    return sims[0] if sims else (None, 0.0)\n",
    "\n",
    "def pick_C(vec: KeyedVectors, a: str, b: str, vocab):\n",
    "    # A ê·¼ë°©ì—ì„œ Bì™€ ë„ˆë¬´ ê°€ê¹ì§€ ì•Šì€ í›„ë³´ë¥¼ ì„ í˜¸\n",
    "    try:\n",
    "        nearA = [w for w, s in vec.most_similar(a, topn=100) if w in vocab and w not in {a, b}]\n",
    "    except KeyError:\n",
    "        nearA = []\n",
    "    random.shuffle(nearA)\n",
    "    for w in nearA:\n",
    "        try:\n",
    "            if vec.similarity(w, b) < 0.6:\n",
    "                return w\n",
    "        except KeyError:\n",
    "            continue\n",
    "    # fallback\n",
    "    rest = [w for w in vocab if w not in {a, b}]\n",
    "    return random.choice(rest) if rest else None\n",
    "\n",
    "def make_round(vec: KeyedVectors, pairs_pool, vocab):\n",
    "    a, b, _ = random.choice(pairs_pool)\n",
    "    c = pick_C(vec, a, b, vocab)\n",
    "    d_pred, conf = best_analogy(vec, a, b, c, vocab)\n",
    "    return {\"A\": a, \"B\": b, \"C\": c, \"D_pred\": d_pred, \"conf\": round(conf, 3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c417cff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_round(rnd: dict) -> str:\n",
    "    return f\"ê´€ê³„ [{rnd['A']}: {rnd['B']} = {rnd['C']} : ?]\"\n",
    "\n",
    "def grade_answer(vec: KeyedVectors, rnd: dict, user_answer: str) -> str:\n",
    "    if rnd[\"D_pred\"] is None or user_answer not in vec:\n",
    "        sim = 0.0\n",
    "    else:\n",
    "        sim = cosine(vec[rnd[\"D_pred\"]], vec[user_answer])\n",
    "    msg = [\n",
    "        render_round(rnd),\n",
    "        f\"ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ê°€ì¥ ì í•©í•œ ë‹¨ì–´: {rnd['D_pred']}\",\n",
    "        f\"ë‹¹ì‹ ì˜ ë‹µë³€ê³¼ ëª¨ë¸ ì˜ˆì¸¡ì˜ ìœ ì‚¬ë„: {sim:.2f}\",\n",
    "    ]\n",
    "    if sim >= 0.7:\n",
    "        msg.append(\"ì •ë‹µì— ë§¤ìš° ê°€ê¹ìŠµë‹ˆë‹¤! ğŸ¯\")\n",
    "    elif sim >= 0.4:\n",
    "        msg.append(\"ì˜¤, ê±°ì˜ ê·¼ì ‘í–ˆì–´ìš”. í•œ ë²ˆë§Œ ë”!\")\n",
    "    else:\n",
    "        msg.append(\"ì•„ì‰½ë„¤ìš”. ë” ìƒê°í•´ë³´ì„¸ìš”.\")\n",
    "    return \"\\n\\n\".join(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e5cf4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê´€ê³„ [ë‚¨ì: ì—¬ì = ë‚¨ì„± : ?]\n",
      "\n",
      "ê´€ê³„ [ë‚¨ì: ì—¬ì = ë‚¨ì„± : ?]\n",
      "\n",
      "ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ê°€ì¥ ì í•©í•œ ë‹¨ì–´: ì—¬ì„±\n",
      "\n",
      "ë‹¹ì‹ ì˜ ë‹µë³€ê³¼ ëª¨ë¸ ì˜ˆì¸¡ì˜ ìœ ì‚¬ë„: 1.00\n",
      "\n",
      "ì •ë‹µì— ë§¤ìš° ê°€ê¹ìŠµë‹ˆë‹¤! ğŸ¯\n"
     ]
    }
   ],
   "source": [
    "rnd = make_round(kv, pairs, candidate_vocab)\n",
    "print(render_round(rnd))\n",
    "\n",
    "user_answer = input(\"ë‹¹ì‹ ì˜ ë‹µì€? \").strip()\n",
    "print(\"\\n\" + grade_answer(kv, rnd, user_answer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
